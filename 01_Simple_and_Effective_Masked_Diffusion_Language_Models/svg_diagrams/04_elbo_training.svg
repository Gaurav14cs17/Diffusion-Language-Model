<?xml version="1.0" encoding="UTF-8"?>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 2200 4000" width="2200" height="4000">
  <defs>
    <linearGradient id="headerGrad" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:#1E40AF;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#3B82F6;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="proofGrad" x1="0%" y1="0%" x2="0%" y2="100%">
      <stop offset="0%" style="stop-color:#EFF6FF;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#DBEAFE;stop-opacity:1" />
    </linearGradient>
  </defs>
  
  <!-- Background -->
  <rect width="2200" height="4000" fill="#FAFAFA"/>
  
  <!-- Main Title Header -->
  <rect x="50" y="40" width="2100" height="120" rx="15" fill="url(#headerGrad)" stroke="#1E3A8A" stroke-width="3"/>
  <text x="1100" y="110" font-family="Georgia, serif" font-size="48" font-weight="bold" fill="white" text-anchor="middle">
    ELBO Derivation &amp; Training Objective
  </text>
  <text x="1100" y="145" font-family="Arial, sans-serif" font-size="22" fill="#BFDBFE" text-anchor="middle">
    The Principled Loss Function for MDLM
  </text>

  <!-- Section 1: Goal -->
  <rect x="50" y="200" width="2100" height="340" rx="12" fill="#EFF6FF" stroke="#3B82F6" stroke-width="3"/>
  <rect x="50" y="200" width="2100" height="60" rx="12" fill="#3B82F6"/>
  <text x="1100" y="242" font-family="Georgia, serif" font-size="36" font-weight="bold" fill="white" text-anchor="middle">
    1. GOAL: MAXIMIZE DATA LIKELIHOOD
  </text>

  <rect x="100" y="290" width="980" height="220" rx="10" fill="#FFFFFF" stroke="#2563EB" stroke-width="3"/>
  <text x="590" y="335" font-family="Georgia, serif" font-size="26" font-weight="bold" fill="#000000" text-anchor="middle">
    Maximum Likelihood Objective
  </text>
  <text x="140" y="390" font-family="Courier New, monospace" font-size="24" font-weight="bold" fill="#000000">
    max_θ  E_{x₀ ~ p_data}[log p_θ(x₀)]
  </text>
  <text x="140" y="445" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    Want model to assign high probability to real data
  </text>
  <text x="140" y="485" font-family="Arial, sans-serif" font-size="20" fill="#1a1a1a">
    But p_θ(x₀) requires intractable marginalization
  </text>

  <rect x="1120" y="290" width="980" height="220" rx="10" fill="#FFFFFF" stroke="#2563EB" stroke-width="3"/>
  <text x="1610" y="335" font-family="Georgia, serif" font-size="26" font-weight="bold" fill="#000000" text-anchor="middle">
    Solution: ELBO
  </text>
  <text x="1160" y="390" font-family="Courier New, monospace" font-size="22" font-weight="bold" fill="#000000">
    log p_θ(x₀) ≥ ELBO(x₀; θ)
  </text>
  <text x="1160" y="445" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    Maximize a tractable lower bound instead
  </text>
  <text x="1160" y="485" font-family="Arial, sans-serif" font-size="20" fill="#1a1a1a">
    ELBO = Evidence Lower BOund
  </text>

  <!-- Section 2: ELBO Derivation -->
  <rect x="50" y="580" width="2100" height="800" rx="12" fill="#F0FDF4" stroke="#22C55E" stroke-width="3"/>
  <rect x="50" y="580" width="2100" height="60" rx="12" fill="#22C55E"/>
  <text x="1100" y="622" font-family="Georgia, serif" font-size="36" font-weight="bold" fill="white" text-anchor="middle">
    2. ELBO DERIVATION FOR MDLM — COMPLETE PROOF
  </text>

  <rect x="100" y="670" width="2000" height="680" rx="10" fill="url(#proofGrad)" stroke="#16A34A" stroke-width="3"/>
  
  <text x="140" y="720" font-family="Georgia, serif" font-size="26" font-weight="bold" fill="#000000">
    Theorem: MDLM Evidence Lower Bound
  </text>
  
  <text x="140" y="775" font-family="Arial, sans-serif" font-size="22" font-weight="bold" fill="#1E3A5F">
    Step 1: Start with Jensen's inequality
  </text>
  <text x="180" y="820" font-family="Courier New, monospace" font-size="18" fill="#000000">
    log p_θ(x₀) = log ∫ p_θ(x₀, x_{0→1}) dx_{0→1}
  </text>
  <text x="180" y="860" font-family="Courier New, monospace" font-size="18" fill="#000000">
               ≥ E_{q(x_{0→1}|x₀)}[log p_θ(x₀, x_{0→1}) - log q(x_{0→1}|x₀)]
  </text>
  
  <text x="140" y="920" font-family="Arial, sans-serif" font-size="22" font-weight="bold" fill="#1E3A5F">
    Step 2: For continuous-time absorbing diffusion
  </text>
  <text x="180" y="965" font-family="Courier New, monospace" font-size="18" fill="#000000">
    ELBO = -∫₀¹ E_{x₀,xₜ}[λ(t) · D_KL(q(x₀|xₜ,x₀) ∥ p_θ(x₀|xₜ))] dt
  </text>
  
  <text x="140" y="1025" font-family="Arial, sans-serif" font-size="22" font-weight="bold" fill="#1E3A5F">
    Step 3: Key simplification — the posterior is deterministic!
  </text>
  <text x="180" y="1070" font-family="Courier New, monospace" font-size="18" fill="#000000">
    q(x₀ⁱ | xₜⁱ, x₀ⁱ) = δ(·, x₀ⁱ)  (we know x₀ⁱ exactly)
  </text>
  <text x="180" y="1110" font-family="Courier New, monospace" font-size="18" fill="#000000">
    So: D_KL(q ∥ p_θ) = -log p_θ(x₀ⁱ | xₜ)  (cross-entropy to one-hot)
  </text>
  
  <text x="140" y="1170" font-family="Arial, sans-serif" font-size="22" font-weight="bold" fill="#1E3A5F">
    Step 4: Only masked positions contribute
  </text>
  <text x="180" y="1215" font-family="Courier New, monospace" font-size="18" fill="#000000">
    If xₜⁱ ≠ [M]: already revealed, no loss (posterior = model = δ(·, xₜⁱ))
  </text>
  <text x="180" y="1255" font-family="Courier New, monospace" font-size="18" fill="#000000">
    If xₜⁱ = [M]: must predict x₀ⁱ from context
  </text>
  
  <rect x="160" y="1285" width="1900" height="50" rx="8" fill="#BBF7D0" stroke="#16A34A" stroke-width="2"/>
  <text x="1100" y="1320" font-family="Georgia, serif" font-size="22" font-weight="bold" fill="#000000" text-anchor="middle">
    Final ELBO: -ELBO = E_{t,x₀,xₜ}[∑_{i: xₜⁱ=[M]} -log p_θ(x₀ⁱ | xₜ)]
  </text>

  <!-- Section 3: Weighting Function -->
  <rect x="50" y="1420" width="2100" height="520" rx="12" fill="#FEF3C7" stroke="#F59E0B" stroke-width="3"/>
  <rect x="50" y="1420" width="2100" height="60" rx="12" fill="#F59E0B"/>
  <text x="1100" y="1462" font-family="Georgia, serif" font-size="36" font-weight="bold" fill="white" text-anchor="middle">
    3. WEIGHTING FUNCTION λ(t)
  </text>

  <rect x="100" y="1510" width="980" height="400" rx="10" fill="#FFFFFF" stroke="#D97706" stroke-width="3"/>
  <text x="590" y="1555" font-family="Georgia, serif" font-size="26" font-weight="bold" fill="#000000" text-anchor="middle">
    Theoretically Optimal Weighting
  </text>
  
  <text x="140" y="1610" font-family="Arial, sans-serif" font-size="22" font-weight="bold" fill="#000000">
    The ELBO-optimal weight:
  </text>
  <text x="160" y="1655" font-family="Courier New, monospace" font-size="22" font-weight="bold" fill="#000000">
    λ(t) = d/dt (1 - αₜ) = 1  (for linear αₜ)
  </text>
  
  <text x="140" y="1720" font-family="Arial, sans-serif" font-size="22" font-weight="bold" fill="#000000">
    Interpretation:
  </text>
  <text x="160" y="1765" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • All timesteps contribute equally
  </text>
  <text x="160" y="1805" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • Sample t ~ Uniform(0, 1)
  </text>
  <text x="160" y="1845" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • No need for importance sampling
  </text>
  <text x="160" y="1885" font-family="Arial, sans-serif" font-size="20" fill="#1a1a1a">
    • Simplest possible scheme
  </text>

  <rect x="1120" y="1510" width="980" height="400" rx="10" fill="#FFFFFF" stroke="#D97706" stroke-width="3"/>
  <text x="1610" y="1555" font-family="Georgia, serif" font-size="26" font-weight="bold" fill="#000000" text-anchor="middle">
    Alternative Weightings
  </text>
  
  <text x="1160" y="1610" font-family="Arial, sans-serif" font-size="22" font-weight="bold" fill="#2563EB">
    SNR Weighting:
  </text>
  <text x="1180" y="1655" font-family="Courier New, monospace" font-size="18" fill="#000000">
    λ(t) ∝ SNR(t) / |SNR'(t)|
  </text>
  <text x="1180" y="1695" font-family="Arial, sans-serif" font-size="18" fill="#1a1a1a">
    Better for perplexity, can hurt sampling
  </text>
  
  <text x="1160" y="1750" font-family="Arial, sans-serif" font-size="22" font-weight="bold" fill="#DC2626">
    Min-SNR-γ Weighting:
  </text>
  <text x="1180" y="1795" font-family="Courier New, monospace" font-size="18" fill="#000000">
    λ(t) = min(SNR(t), γ)
  </text>
  <text x="1180" y="1835" font-family="Arial, sans-serif" font-size="18" fill="#1a1a1a">
    Clips extreme weights, balances both
  </text>
  
  <text x="1160" y="1890" font-family="Arial, sans-serif" font-size="22" font-weight="bold" fill="#22C55E">
    MDLM finding: Uniform works great!
  </text>

  <!-- Section 4: Final Training Loss -->
  <rect x="50" y="1980" width="2100" height="620" rx="12" fill="#FAF5FF" stroke="#A855F7" stroke-width="3"/>
  <rect x="50" y="1980" width="2100" height="60" rx="12" fill="#A855F7"/>
  <text x="1100" y="2022" font-family="Georgia, serif" font-size="36" font-weight="bold" fill="white" text-anchor="middle">
    4. FINAL TRAINING LOSS
  </text>

  <rect x="100" y="2070" width="2000" height="500" rx="10" fill="#FFFFFF" stroke="#7C3AED" stroke-width="3"/>
  
  <text x="1100" y="2120" font-family="Georgia, serif" font-size="28" font-weight="bold" fill="#000000" text-anchor="middle">
    MDLM Training Objective
  </text>
  
  <rect x="160" y="2150" width="1880" height="100" rx="10" fill="#FAF5FF" stroke="#A855F7" stroke-width="3"/>
  <text x="1100" y="2195" font-family="Courier New, monospace" font-size="22" font-weight="bold" fill="#000000" text-anchor="middle">
    L_MDLM = E_{t~U(0,1), x₀~p_data, xₜ~q(·|x₀)}
  </text>
  <text x="1100" y="2235" font-family="Courier New, monospace" font-size="22" font-weight="bold" fill="#000000" text-anchor="middle">
    [ ∑_{i: xₜⁱ = [MASK]} -log p_θ(x₀ⁱ | xₜ, t) ]
  </text>
  
  <text x="140" y="2305" font-family="Arial, sans-serif" font-size="24" font-weight="bold" fill="#000000">
    In words:
  </text>
  <text x="180" y="2355" font-family="Arial, sans-serif" font-size="22" fill="#000000">
    1. Sample random time t uniformly from [0, 1]
  </text>
  <text x="180" y="2400" font-family="Arial, sans-serif" font-size="22" fill="#000000">
    2. Sample clean data x₀ from training distribution
  </text>
  <text x="180" y="2445" font-family="Arial, sans-serif" font-size="22" fill="#000000">
    3. Create xₜ by masking each token independently with probability t
  </text>
  <text x="180" y="2490" font-family="Arial, sans-serif" font-size="22" fill="#000000">
    4. Compute cross-entropy loss ONLY on masked positions
  </text>
  <text x="180" y="2535" font-family="Arial, sans-serif" font-size="22" fill="#000000">
    5. Average over batch and update θ
  </text>

  <!-- Section 5: Comparison with BERT -->
  <rect x="50" y="2640" width="2100" height="460" rx="12" fill="#ECFDF5" stroke="#10B981" stroke-width="3"/>
  <rect x="50" y="2640" width="2100" height="60" rx="12" fill="#10B981"/>
  <text x="1100" y="2682" font-family="Georgia, serif" font-size="36" font-weight="bold" fill="white" text-anchor="middle">
    5. COMPARISON: MDLM vs BERT MLM
  </text>

  <!-- Comparison boxes -->
  <rect x="100" y="2730" width="980" height="340" rx="10" fill="#FFFFFF" stroke="#059669" stroke-width="3"/>
  <text x="590" y="2775" font-family="Georgia, serif" font-size="26" font-weight="bold" fill="#000000" text-anchor="middle">
    BERT MLM
  </text>
  
  <text x="140" y="2825" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • Fixed 15% masking rate
  </text>
  <text x="140" y="2865" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • 80% [MASK], 10% random, 10% keep
  </text>
  <text x="140" y="2905" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • Heuristic design choices
  </text>
  <text x="140" y="2945" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • No principled objective
  </text>
  <text x="140" y="2985" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • Not designed for generation
  </text>
  <text x="140" y="3025" font-family="Arial, sans-serif" font-size="20" fill="#DC2626" font-weight="bold">
    • No likelihood bound
  </text>

  <rect x="1120" y="2730" width="980" height="340" rx="10" fill="#FFFFFF" stroke="#059669" stroke-width="3"/>
  <text x="1610" y="2775" font-family="Georgia, serif" font-size="26" font-weight="bold" fill="#000000" text-anchor="middle">
    MDLM (This Paper)
  </text>
  
  <text x="1160" y="2825" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • Variable masking rate t ∈ [0,1]
  </text>
  <text x="1160" y="2865" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • Always [MASK] (absorbing state)
  </text>
  <text x="1160" y="2905" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • Principled, derived from ELBO
  </text>
  <text x="1160" y="2945" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • Theoretically justified
  </text>
  <text x="1160" y="2985" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • Designed for generation
  </text>
  <text x="1160" y="3025" font-family="Arial, sans-serif" font-size="20" fill="#22C55E" font-weight="bold">
    • Tight likelihood bound!
  </text>

  <!-- Section 6: Training Algorithm -->
  <rect x="50" y="3140" width="2100" height="520" rx="12" fill="#FEF2F2" stroke="#EF4444" stroke-width="3"/>
  <rect x="50" y="3140" width="2100" height="60" rx="12" fill="#EF4444"/>
  <text x="1100" y="3182" font-family="Georgia, serif" font-size="36" font-weight="bold" fill="white" text-anchor="middle">
    6. COMPLETE TRAINING ALGORITHM
  </text>

  <rect x="100" y="3230" width="2000" height="400" rx="10" fill="#FFFFFF" stroke="#DC2626" stroke-width="3"/>
  
  <text x="1100" y="3280" font-family="Georgia, serif" font-size="26" font-weight="bold" fill="#000000" text-anchor="middle">
    Algorithm: MDLM Training Loop
  </text>
  
  <rect x="140" y="3310" width="1920" height="290" rx="8" fill="#FEF2F2" stroke="#DC2626" stroke-width="2"/>
  
  <text x="180" y="3355" font-family="Courier New, monospace" font-size="20" font-weight="bold" fill="#DC2626">
    Input: Dataset D, model f_θ, learning rate η
  </text>
  
  <text x="180" y="3400" font-family="Courier New, monospace" font-size="18" fill="#000000">
    repeat until convergence:
  </text>
  <text x="220" y="3440" font-family="Courier New, monospace" font-size="18" fill="#000000">
    1. Sample batch x₀ ~ D
  </text>
  <text x="220" y="3480" font-family="Courier New, monospace" font-size="18" fill="#000000">
    2. Sample t ~ Uniform(0, 1) for each example
  </text>
  <text x="220" y="3520" font-family="Courier New, monospace" font-size="18" fill="#000000">
    3. Create xₜ: for each position, mask with probability t
  </text>
  <text x="220" y="3560" font-family="Courier New, monospace" font-size="18" fill="#000000">
    4. L = mean(∑_{masked positions} -log softmax(f_θ(xₜ, t))_{x₀})
  </text>
  <text x="220" y="3600" font-family="Courier New, monospace" font-size="18" fill="#000000">
    5. θ ← θ - η · ∇_θ L
  </text>

  <!-- Footer -->
  <rect x="50" y="3700" width="2100" height="260" rx="12" fill="#1E3A5F" stroke="#0F172A" stroke-width="3"/>
  <text x="1100" y="3755" font-family="Georgia, serif" font-size="32" font-weight="bold" fill="white" text-anchor="middle">
    Key Takeaways
  </text>
  
  <text x="300" y="3815" font-family="Arial, sans-serif" font-size="20" fill="#93C5FD">
    ✓ ELBO provides theoretical foundation
  </text>
  <text x="300" y="3855" font-family="Arial, sans-serif" font-size="20" fill="#93C5FD">
    ✓ Cross-entropy on masked positions only
  </text>
  <text x="300" y="3895" font-family="Arial, sans-serif" font-size="20" fill="#93C5FD">
    ✓ Uniform time sampling is optimal
  </text>
  <text x="300" y="3935" font-family="Arial, sans-serif" font-size="20" fill="#93C5FD">
    ✓ Simple = BERT MLM with variable rate
  </text>
  
  <text x="1100" y="3815" font-family="Arial, sans-serif" font-size="20" fill="#93C5FD">
    ✓ No auxiliary losses needed
  </text>
  <text x="1100" y="3855" font-family="Arial, sans-serif" font-size="20" fill="#93C5FD">
    ✓ Tight lower bound on log p(x₀)
  </text>
  <text x="1100" y="3895" font-family="Arial, sans-serif" font-size="20" fill="#93C5FD">
    ✓ Principled design vs heuristic BERT
  </text>
  <text x="1100" y="3935" font-family="Arial, sans-serif" font-size="20" fill="#93C5FD">
    ✓ Enables proper generation
  </text>
</svg>

