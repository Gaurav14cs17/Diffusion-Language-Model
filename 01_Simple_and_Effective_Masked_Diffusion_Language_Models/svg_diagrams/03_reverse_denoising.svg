<?xml version="1.0" encoding="UTF-8"?>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 2200 3600" width="2200" height="3600">
  <defs>
    <linearGradient id="headerGrad" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:#166534;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#22C55E;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="proofGrad" x1="0%" y1="0%" x2="0%" y2="100%">
      <stop offset="0%" style="stop-color:#F0FDF4;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#DCFCE7;stop-opacity:1" />
    </linearGradient>
  </defs>
  
  <!-- Background -->
  <rect width="2200" height="3600" fill="#FAFAFA"/>
  
  <!-- Main Title Header -->
  <rect x="50" y="40" width="2100" height="120" rx="15" fill="url(#headerGrad)" stroke="#14532D" stroke-width="3"/>
  <text x="1100" y="110" font-family="Georgia, serif" font-size="48" font-weight="bold" fill="white" text-anchor="middle">
    Reverse Denoising Process &amp; Parameterization
  </text>
  <text x="1100" y="145" font-family="Arial, sans-serif" font-size="22" fill="#BBF7D0" text-anchor="middle">
    Learning to Unmask: p_θ(x₀ | xₜ) in MDLM
  </text>

  <!-- Section 1: Goal -->
  <rect x="50" y="200" width="2100" height="380" rx="12" fill="#F0FDF4" stroke="#22C55E" stroke-width="3"/>
  <rect x="50" y="200" width="2100" height="60" rx="12" fill="#22C55E"/>
  <text x="1100" y="242" font-family="Georgia, serif" font-size="36" font-weight="bold" fill="white" text-anchor="middle">
    1. GOAL: REVERSE THE MASKING PROCESS
  </text>

  <rect x="100" y="290" width="980" height="260" rx="10" fill="#FFFFFF" stroke="#16A34A" stroke-width="3"/>
  <text x="590" y="335" font-family="Georgia, serif" font-size="26" font-weight="bold" fill="#000000" text-anchor="middle">
    What We Want
  </text>
  <text x="140" y="385" font-family="Arial, sans-serif" font-size="22" fill="#000000">
    Given: Partially masked sequence xₜ
  </text>
  <text x="140" y="425" font-family="Arial, sans-serif" font-size="22" fill="#000000">
    Predict: Original tokens at masked positions
  </text>
  <text x="140" y="475" font-family="Courier New, monospace" font-size="22" font-weight="bold" fill="#000000">
    p_θ(x₀ | xₜ) = ?
  </text>
  <text x="140" y="525" font-family="Arial, sans-serif" font-size="20" fill="#1a1a1a">
    Learn a neural network to predict x₀ from xₜ
  </text>

  <rect x="1120" y="290" width="980" height="260" rx="10" fill="#FFFFFF" stroke="#16A34A" stroke-width="3"/>
  <text x="1610" y="335" font-family="Georgia, serif" font-size="26" font-weight="bold" fill="#000000" text-anchor="middle">
    Key Insight
  </text>
  <text x="1160" y="385" font-family="Arial, sans-serif" font-size="22" fill="#000000">
    For masked positions: predict original token
  </text>
  <text x="1160" y="425" font-family="Arial, sans-serif" font-size="22" fill="#000000">
    For unmasked positions: keep as is
  </text>
  <text x="1160" y="475" font-family="Courier New, monospace" font-size="20" font-weight="bold" fill="#000000">
    If xₜⁱ ≠ [M]: p_θ(x₀ⁱ|xₜ) = δ(x₀ⁱ, xₜⁱ)
  </text>
  <text x="1160" y="525" font-family="Arial, sans-serif" font-size="20" fill="#1a1a1a">
    Only need predictions for [MASK] tokens!
  </text>

  <!-- Section 2: Posterior Distribution -->
  <rect x="50" y="620" width="2100" height="620" rx="12" fill="#FEF3C7" stroke="#F59E0B" stroke-width="3"/>
  <rect x="50" y="620" width="2100" height="60" rx="12" fill="#F59E0B"/>
  <text x="1100" y="662" font-family="Georgia, serif" font-size="36" font-weight="bold" fill="white" text-anchor="middle">
    2. POSTERIOR DISTRIBUTION q(xₜ₋ₛ | xₜ, x₀)
  </text>

  <rect x="100" y="710" width="2000" height="500" rx="10" fill="url(#proofGrad)" stroke="#D97706" stroke-width="3"/>
  
  <text x="140" y="760" font-family="Georgia, serif" font-size="26" font-weight="bold" fill="#000000">
    Theorem: Tractable Posterior for Absorbing Diffusion
  </text>
  
  <text x="140" y="815" font-family="Arial, sans-serif" font-size="22" font-weight="bold" fill="#1E3A5F">
    Given xₜ and x₀, the posterior at time t-s has a closed form:
  </text>
  
  <!-- Case 1 -->
  <rect x="160" y="850" width="900" height="140" rx="8" fill="#BBF7D0" stroke="#16A34A" stroke-width="2"/>
  <text x="610" y="890" font-family="Georgia, serif" font-size="22" font-weight="bold" fill="#000000" text-anchor="middle">
    Case 1: xₜⁱ ≠ [MASK] (token is visible)
  </text>
  <text x="200" y="930" font-family="Courier New, monospace" font-size="20" font-weight="bold" fill="#000000">
    q(xₜ₋ₛⁱ | xₜⁱ, x₀ⁱ) = δ(xₜ₋ₛⁱ, xₜⁱ) = δ(xₜ₋ₛⁱ, x₀ⁱ)
  </text>
  <text x="200" y="970" font-family="Arial, sans-serif" font-size="18" fill="#1a1a1a">
    Token was never masked → stays as original
  </text>

  <!-- Case 2 -->
  <rect x="1100" y="850" width="950" height="140" rx="8" fill="#FECACA" stroke="#DC2626" stroke-width="2"/>
  <text x="1575" y="890" font-family="Georgia, serif" font-size="22" font-weight="bold" fill="#000000" text-anchor="middle">
    Case 2: xₜⁱ = [MASK] (token is masked)
  </text>
  <text x="1140" y="930" font-family="Courier New, monospace" font-size="18" font-weight="bold" fill="#000000">
    q(xₜ₋ₛⁱ|xₜⁱ=[M],x₀ⁱ) = θₜ,ₛ·δ(xₜ₋ₛⁱ,x₀ⁱ) + (1-θₜ,ₛ)·δ(xₜ₋ₛⁱ,[M])
  </text>
  <text x="1140" y="970" font-family="Arial, sans-serif" font-size="18" fill="#1a1a1a">
    Either unmask (prob θ) or stay masked
  </text>

  <text x="140" y="1040" font-family="Georgia, serif" font-size="24" font-weight="bold" fill="#000000">
    Unmasking Probability θₜ,ₛ:
  </text>
  
  <rect x="160" y="1070" width="1900" height="80" rx="8" fill="#FFFFFF" stroke="#4A5568" stroke-width="2"/>
  <text x="1100" y="1120" font-family="Courier New, monospace" font-size="24" font-weight="bold" fill="#000000" text-anchor="middle">
    θₜ,ₛ = (αₜ₋ₛ - αₜ) / (1 - αₜ) = s / t   (for linear αₜ = 1-t)
  </text>
  
  <text x="140" y="1180" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • θₜ,ₛ = probability of revealing a token in this step
  </text>
  <text x="140" y="1175" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    
  </text>

  <!-- Section 3: Neural Network -->
  <rect x="50" y="1280" width="2100" height="600" rx="12" fill="#EFF6FF" stroke="#3B82F6" stroke-width="3"/>
  <rect x="50" y="1280" width="2100" height="60" rx="12" fill="#3B82F6"/>
  <text x="1100" y="1322" font-family="Georgia, serif" font-size="36" font-weight="bold" fill="white" text-anchor="middle">
    3. NEURAL NETWORK PARAMETERIZATION
  </text>

  <rect x="100" y="1370" width="980" height="480" rx="10" fill="#FFFFFF" stroke="#2563EB" stroke-width="3"/>
  <text x="590" y="1415" font-family="Georgia, serif" font-size="26" font-weight="bold" fill="#000000" text-anchor="middle">
    x₀-Prediction (MDLM Choice)
  </text>
  
  <text x="140" y="1465" font-family="Arial, sans-serif" font-size="22" font-weight="bold" fill="#000000">
    Network predicts clean tokens:
  </text>
  <text x="160" y="1510" font-family="Courier New, monospace" font-size="22" font-weight="bold" fill="#000000">
    f_θ(xₜ, t) → logits ∈ ℝⁿˣ|V|
  </text>
  
  <text x="140" y="1570" font-family="Arial, sans-serif" font-size="22" font-weight="bold" fill="#000000">
    For each position i:
  </text>
  <text x="160" y="1615" font-family="Courier New, monospace" font-size="20" fill="#000000">
    p_θ(x₀ⁱ = v | xₜ) = softmax(f_θ(xₜ,t)ⁱ)ᵥ
  </text>
  
  <text x="140" y="1675" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • Predicts distribution over vocabulary V
  </text>
  <text x="140" y="1715" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • Same as BERT's masked LM head
  </text>
  <text x="140" y="1755" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • But with time t as additional input
  </text>
  <text x="140" y="1795" font-family="Arial, sans-serif" font-size="20" fill="#1a1a1a">
    • Bidirectional attention (full context)
  </text>

  <rect x="1120" y="1370" width="980" height="480" rx="10" fill="#FFFFFF" stroke="#2563EB" stroke-width="3"/>
  <text x="1610" y="1415" font-family="Georgia, serif" font-size="26" font-weight="bold" fill="#000000" text-anchor="middle">
    Architecture Details
  </text>
  
  <text x="1160" y="1465" font-family="Arial, sans-serif" font-size="22" font-weight="bold" fill="#000000">
    Input representation:
  </text>
  <text x="1180" y="1510" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    h₀ = Embed(xₜ) + PosEmbed + TimeEmbed(t)
  </text>
  
  <text x="1160" y="1570" font-family="Arial, sans-serif" font-size="22" font-weight="bold" fill="#000000">
    Time embedding:
  </text>
  <text x="1180" y="1615" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    Sinusoidal or learned embedding of t
  </text>
  <text x="1180" y="1655" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    Added to each position's representation
  </text>
  
  <text x="1160" y="1715" font-family="Arial, sans-serif" font-size="22" font-weight="bold" fill="#000000">
    Key difference from GPT:
  </text>
  <text x="1180" y="1760" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • NO causal attention mask
  </text>
  <text x="1180" y="1800" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • All positions attend to all others
  </text>

  <!-- Section 4: Reverse Step Computation -->
  <rect x="50" y="1920" width="2100" height="560" rx="12" fill="#FAF5FF" stroke="#A855F7" stroke-width="3"/>
  <rect x="50" y="1920" width="2100" height="60" rx="12" fill="#A855F7"/>
  <text x="1100" y="1962" font-family="Georgia, serif" font-size="36" font-weight="bold" fill="white" text-anchor="middle">
    4. COMPUTING THE REVERSE STEP
  </text>

  <rect x="100" y="2010" width="2000" height="440" rx="10" fill="#FFFFFF" stroke="#7C3AED" stroke-width="3"/>
  
  <text x="1100" y="2060" font-family="Georgia, serif" font-size="26" font-weight="bold" fill="#000000" text-anchor="middle">
    Computing p_θ(xₜ₋ₛ | xₜ) using predicted x̂₀
  </text>
  
  <text x="140" y="2115" font-family="Arial, sans-serif" font-size="22" font-weight="bold" fill="#1E3A5F">
    Step 1: Get model predictions
  </text>
  <text x="180" y="2155" font-family="Courier New, monospace" font-size="20" fill="#000000">
    logits = f_θ(xₜ, t)  →  p_θ(x₀ | xₜ) = softmax(logits)
  </text>
  
  <text x="140" y="2210" font-family="Arial, sans-serif" font-size="22" font-weight="bold" fill="#1E3A5F">
    Step 2: For each position i
  </text>
  <text x="180" y="2250" font-family="Courier New, monospace" font-size="18" fill="#000000">
    If xₜⁱ ≠ [M]:  p_θ(xₜ₋ₛⁱ | xₜ) = δ(xₜ₋ₛⁱ, xₜⁱ)  (keep as is)
  </text>
  
  <text x="180" y="2295" font-family="Courier New, monospace" font-size="18" fill="#000000">
    If xₜⁱ = [M]:  p_θ(xₜ₋ₛⁱ | xₜ) = θₜ,ₛ · p_θ(x₀ⁱ | xₜ) + (1-θₜ,ₛ) · δ(xₜ₋ₛⁱ, [M])
  </text>
  
  <text x="140" y="2355" font-family="Arial, sans-serif" font-size="22" font-weight="bold" fill="#1E3A5F">
    Step 3: Sample xₜ₋ₛⁱ from the computed distribution
  </text>
  <text x="180" y="2395" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • With prob θₜ,ₛ: sample from p_θ(x₀ⁱ | xₜ) → reveals token
  </text>
  <text x="180" y="2435" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    • With prob 1-θₜ,ₛ: keep as [MASK] → stays masked
  </text>

  <!-- Section 5: Why x₀-Prediction -->
  <rect x="50" y="2520" width="2100" height="460" rx="12" fill="#ECFDF5" stroke="#10B981" stroke-width="3"/>
  <rect x="50" y="2520" width="2100" height="60" rx="12" fill="#10B981"/>
  <text x="1100" y="2562" font-family="Georgia, serif" font-size="36" font-weight="bold" fill="white" text-anchor="middle">
    5. WHY x₀-PREDICTION WORKS BEST
  </text>

  <rect x="100" y="2610" width="650" height="340" rx="10" fill="#FFFFFF" stroke="#059669" stroke-width="3"/>
  <text x="425" y="2655" font-family="Georgia, serif" font-size="24" font-weight="bold" fill="#000000" text-anchor="middle">
    Alternative: ε-Prediction
  </text>
  <text x="140" y="2705" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    Predict noise instead of clean data
  </text>
  <text x="140" y="2750" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    Works for continuous diffusion (images)
  </text>
  <text x="140" y="2805" font-family="Arial, sans-serif" font-size="20" font-weight="bold" fill="#DC2626">
    Problem for discrete:
  </text>
  <text x="160" y="2845" font-family="Arial, sans-serif" font-size="18" fill="#000000">
    • No gradient for discrete tokens
  </text>
  <text x="160" y="2880" font-family="Arial, sans-serif" font-size="18" fill="#000000">
    • "Noise" is just [MASK], not Gaussian
  </text>
  <text x="160" y="2915" font-family="Arial, sans-serif" font-size="18" fill="#1a1a1a">
    • Can't compute x₀ = xₜ - ε
  </text>

  <rect x="780" y="2610" width="650" height="340" rx="10" fill="#FFFFFF" stroke="#059669" stroke-width="3"/>
  <text x="1105" y="2655" font-family="Georgia, serif" font-size="24" font-weight="bold" fill="#000000" text-anchor="middle">
    x₀-Prediction (MDLM)
  </text>
  <text x="820" y="2705" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    Directly predict original tokens
  </text>
  <text x="820" y="2750" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    Natural for discrete/categorical data
  </text>
  <text x="820" y="2805" font-family="Arial, sans-serif" font-size="20" font-weight="bold" fill="#22C55E">
    Advantages:
  </text>
  <text x="840" y="2845" font-family="Arial, sans-serif" font-size="18" fill="#000000">
    • Directly outputs token probabilities
  </text>
  <text x="840" y="2880" font-family="Arial, sans-serif" font-size="18" fill="#000000">
    • Same as language modeling
  </text>
  <text x="840" y="2915" font-family="Arial, sans-serif" font-size="18" fill="#000000">
    • Simple cross-entropy loss
  </text>

  <rect x="1460" y="2610" width="590" height="340" rx="10" fill="#FFFFFF" stroke="#059669" stroke-width="3"/>
  <text x="1755" y="2655" font-family="Georgia, serif" font-size="24" font-weight="bold" fill="#000000" text-anchor="middle">
    Connection to BERT
  </text>
  <text x="1500" y="2705" font-family="Arial, sans-serif" font-size="20" fill="#000000">
    MDLM = Principled BERT
  </text>
  <text x="1500" y="2760" font-family="Arial, sans-serif" font-size="18" fill="#000000">
    • BERT: fixed 15% masking
  </text>
  <text x="1500" y="2800" font-family="Arial, sans-serif" font-size="18" fill="#000000">
    • MDLM: variable masking
  </text>
  <text x="1520" y="2835" font-family="Arial, sans-serif" font-size="18" fill="#000000">
    rate t ∈ [0,1]
  </text>
  <text x="1500" y="2880" font-family="Arial, sans-serif" font-size="18" fill="#000000">
    • BERT: heuristic objective
  </text>
  <text x="1500" y="2920" font-family="Arial, sans-serif" font-size="18" fill="#000000">
    • MDLM: ELBO-derived
  </text>

  <!-- Footer -->
  <rect x="50" y="3020" width="2100" height="540" rx="12" fill="#1E3A5F" stroke="#0F172A" stroke-width="3"/>
  <text x="1100" y="3080" font-family="Georgia, serif" font-size="32" font-weight="bold" fill="white" text-anchor="middle">
    Complete Reverse Process Summary
  </text>
  
  <rect x="100" y="3110" width="2000" height="420" rx="10" fill="#F1F5F9" stroke="#334155" stroke-width="2"/>
  
  <text x="140" y="3160" font-family="Courier New, monospace" font-size="20" font-weight="bold" fill="#3B82F6">
    Algorithm: One Reverse Step (t → t-s)
  </text>
  
  <text x="180" y="3210" font-family="Courier New, monospace" font-size="18" fill="#000000">
    1. Get predictions: logits = f_θ(xₜ, t), probs = softmax(logits)
  </text>
  <text x="180" y="3255" font-family="Courier New, monospace" font-size="18" fill="#000000">
    2. Compute unmasking probability: θ = s/t
  </text>
  <text x="180" y="3300" font-family="Courier New, monospace" font-size="18" fill="#000000">
    3. For each position i:
  </text>
  <text x="220" y="3345" font-family="Courier New, monospace" font-size="18" fill="#000000">
    If xₜⁱ ≠ [M]: xₜ₋ₛⁱ = xₜⁱ  (already revealed, keep it)
  </text>
  <text x="220" y="3390" font-family="Courier New, monospace" font-size="18" fill="#000000">
    If xₜⁱ = [M]:
  </text>
  <text x="260" y="3435" font-family="Courier New, monospace" font-size="18" fill="#000000">
    Sample u ~ Uniform(0,1)
  </text>
  <text x="260" y="3480" font-family="Courier New, monospace" font-size="18" fill="#000000">
    If u &lt; θ: xₜ₋ₛⁱ ~ Categorical(probs[i])  (reveal token)
  </text>
  <text x="260" y="3520" font-family="Courier New, monospace" font-size="18" fill="#000000">
    Else: xₜ₋ₛⁱ = [M]  (stay masked)
  </text>
</svg>

