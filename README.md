<div align="center">

# ğŸŒ€ Diffusion Language Models

<img src="https://readme-typing-svg.demolab.com?font=Fira+Code&weight=600&size=28&duration=3000&pause=1000&color=9D4EDD&center=true&vCenter=true&multiline=true&repeat=false&width=800&height=100&lines=From+Noise+to+Language;The+Future+of+Text+Generation" alt="Typing SVG" />

<br>

[![Diffusion](https://img.shields.io/badge/ğŸ”„_Diffusion-Language_Models-9D4EDD?style=for-the-badge&labelColor=1a1a2e)](.)
[![Research](https://img.shields.io/badge/ğŸ“š_Research-Papers-16a085?style=for-the-badge&labelColor=1a1a2e)](.)
[![Math](https://img.shields.io/badge/ğŸ§®_Complete-Proofs-e74c3c?style=for-the-badge&labelColor=1a1a2e)](.)

<br>

*A deep dive into the revolutionary paradigm of iterative text generation*

---

### ğŸ¬ The Diffusion Process

```
.--------------------------------------------------------------------------.
|                                                                          |
|   â¬‡ï¸  FORWARD (Corruption)                                               |
|   ====================================================================   |
|                                                                          |
|   "The cat sat on the mat"                                               |
|            â†“                                                             |
|   "The [M] sat on the mat"                                               |
|            â†“                                                             |
|   "[M] [M] sat [M] the [M]"                                              |
|            â†“                                                             |
|   "[M] [M] [M] [M] [M] [M]"  â† Pure Noise                                |
|                                                                          |
|   â¬†ï¸  REVERSE (Generation)                                               |
|   ====================================================================   |
|                                                                          |
|   "[M] [M] [M] [M] [M] [M]"  â† Start with noise                          |
|            â†“                                                             |
|   "[M] cat [M] [M] [M] mat"                                              |
|            â†“                                                             |
|   "The cat sat [M] the mat"                                              |
|            â†“                                                             |
|   "The cat sat on the mat"  â† Clean text!                                |
|                                                                          |
'--------------------------------------------------------------------------'

```

</div>

---

## ğŸŒŸ Why Diffusion for Language?

<table>
<tr>
<td width="50%">

### âŒ Autoregressive Limitations

```
Token by token, left to right...
Never looking back...

[The] â†’ [The][cat] â†’ [The][cat][sat]

â€¢ Can't revise past decisions
â€¢ Only sees left context  
â€¢ Sequential = slow for long text

```

</td>
<td width="50%">

### âœ… Diffusion Advantages

```
All at once, refine iteratively...
Full context, full power...

[M][M][M] â†’ [The][M][sat] â†’ [The][cat][sat]

â€¢ Revise any position anytime
â€¢ Bidirectional context
â€¢ Parallel = fast generation

```

</td>
</tr>
</table>

---

## ğŸ“š The Learning Journey

<div align="center">

```
                    +-------------------------------------+
                    |         START YOUR JOURNEY          |
                    |              HERE â†“                 |
                    +-------------------------------------+
                                     |
                    +================â•§================+
                    |   ğŸ“ 00_Diffusion_Theory        |
                    |   -------------------------     |
                    |   Master the fundamentals:      |
                    |   â€¢ Data Space & Probability    |
                    |   â€¢ Forward & Reverse Process   |
                    |   â€¢ ELBO & Training Objective   |
                    |   â€¢ Sampling Algorithms         |
                    +================â•¤================+
                                     |
                    +================â•§================+
                    |   ğŸ­ 01_MDLM                    |
                    |   -------------------------     |
                    |   Simple & Effective Masked     |
                    |   Diffusion Language Models     |
                    |   â€¢ Absorbing state diffusion   |
                    |   â€¢ Simplified ELBO             |
                    +================â•¤================+
                                     |
                    +================â•§================+
                    |   ğŸ”€ 02_Block_Diffusion         |
                    |   -------------------------     |
                    |   Best of Both Worlds:          |
                    |   AR + Diffusion Hybrid         |
                    |   â€¢ Block-causal attention      |
                    |   â€¢ KV cache compatibility      |
                    +================â•¤================+
                                     |
                    +================â•§================+
                    |   ğŸš€ 03_Dream_7B                |
                    |   -------------------------     |
                    |   Scaling Up Diffusion LLMs     |
                    |   â€¢ Large-scale training        |
                    |   â€¢ Competitive with AR         |
                    +================â•¤================+
                                     |
                    +================â•§================+
                    |   âš¡ 04_LLaDA_MoE               |
                    |   -------------------------     |
                    |   Efficiency through Sparsity   |
                    |   â€¢ Mixture of Experts          |
                    |   â€¢ Compute-efficient inference |
                    +================================+
                                     |
                    +----------------+----------------+
                    |     ğŸ“ YOU'RE NOW AN EXPERT!    |
                    +---------------------------------+

```

</div>

---

## ğŸ“‚ Repository Map

```
ğŸ“¦ Diffusion Language Model
|
+-- ğŸ“ 00_Diffusion_Theory/
|   |
|   +-- 01 Data Space/              â†’ What is our playground?
|   +-- 02 Probability Assumptions/ â†’ The math foundations
|   +-- 03 Gaussian Transition/     â†’ How noise is added
|   +-- 04 Forward Process/         â†’ Corruption step-by-step
|   +-- 05 Noise Schedule/          â†’ Controlling the chaos
|   +-- 06 Marginal Distributions/  â†’ Any-time noise levels
|   +-- 07 Reverse Process/         â†’ Learning to denoise
|   +-- 08 Training Objective/      â†’ What we optimize
|   +-- 09 Parameterization/        â†’ Network design choices
|   +-- 10 Sampling/                â†’ Generation algorithms
|
+-- ğŸ“„ 01_Simple_and_Effective_MDLM/
|   +-- ğŸ“– README.md                â†’ Detailed blog post
|   +-- ğŸ¨ svg_diagrams/            â†’ Visual explanations
|   +-- ğŸ“‘ paper.pdf                â†’ Original paper
|
+-- ğŸ“„ 02_Block_Diffusion/
|   +-- ğŸ“– README.md                â†’ Detailed blog post
|   +-- ğŸ¨ images/                  â†’ Visual explanations
|   +-- ğŸ“‘ paper.pdf                â†’ Original paper
|
+-- ğŸ“„ 03_Dream_7B/
|   +-- ğŸ“– README.md                â†’ Detailed blog post
|   +-- ğŸ¨ svg_diagrams/            â†’ Visual explanations
|   +-- ğŸ“‘ paper.pdf                â†’ Original paper
|
+-- ğŸ“„ 04_LLaDA_MoE/
    +-- ğŸ“– README.md                â†’ Detailed blog post
    +-- ğŸ¨ images/                  â†’ Visual explanations
    +-- ğŸ“‘ paper.pdf                â†’ Original paper

```

---

## ğŸ§® The Core Mathematics

<div align="center">

### Forward Process: Adding Noise

```
.--------------------------------------------------------------------.
|                                                                    |
|   At noise level t âˆˆ [0, 1], each token independently:            |
|                                                                    |
|   +---------------------------------------------------------+     |
|   |                                                         |     |
|   |   q(yâ‚œâ± | yâ±) = + (1-t)  â†’  keep original token        |     |
|   |                 |                                       |     |
|   |                 +  t     â†’  replace with [MASK]        |     |
|   |                                                         |     |
|   +---------------------------------------------------------+     |
|                                                                    |
|   t = 0.0  â†’  "The quick brown fox"        (clean)                |
|   t = 0.3  â†’  "The [M] brown [M]"          (some masked)          |
|   t = 0.7  â†’  "[M] [M] [M] fox"            (mostly masked)        |
|   t = 1.0  â†’  "[M] [M] [M] [M]"            (fully masked)         |
|                                                                    |
'--------------------------------------------------------------------'

```

### Training Objective: ELBO

```
.--------------------------------------------------------------------.
|                                                                    |
|   Minimize the negative log-likelihood bound:                      |
|                                                                    |
|   +---------------------------------------------------------+     |
|   |                                                         |     |
|   |   ğ“›(Î¸) = âˆ’ğ”¼   ğ”¼      ğ”¼     [ 1/t Â· Î£áµ¢ ğŸ™[yâ‚œâ±=M]        |     |
|   |           y   t~U    yâ‚œ                                 |     |
|   |                                                         |     |
|   |                            Â· log pÎ¸(yâ± | yâ‚œ) ]         |     |
|   |                                                         |     |
|   +---------------------------------------------------------+     |
|                                                                    |
|   Key insight: 1/t weighting makes gradients unbiased!            |
|                                                                    |
'--------------------------------------------------------------------'

```

</div>

---

## ğŸ“Š Paper Overview

<div align="center">

| | Paper | Innovation | Key Idea |
|:---:|:---|:---|:---|
| ğŸ­ | **MDLM** | Masked Diffusion | Absorbing state with simplified training |
| ğŸ”€ | **Block Diffusion** | Hybrid AR+Diffusion | Blocks for AR, diffusion within blocks |
| ğŸš€ | **Dream 7B** | Scaling Laws | First competitive large diffusion LLM |
| âš¡ | **LLaDA-MoE** | Sparse Efficiency | MoE architecture for fast inference |

</div>

---

## ğŸ”— Resources & Links

<div align="center">

| Paper | arXiv | Code/Models |
|:------|:-----:|:-----------:|
| Simple & Effective MDLM | [ğŸ“„ 2406.07524](https://arxiv.org/abs/2406.07524) | â€” |
| Block Diffusion | [ğŸ“„ 2503.09573](https://arxiv.org/abs/2503.09573) | â€” |
| Dream 7B | [ğŸ“„ 2508.15487](https://arxiv.org/abs/2508.15487) | [ğŸ¤— HuggingFace](https://huggingface.co/Dream-org) |
| LLaDA-MoE | [ğŸ“„ 2509.24389](https://arxiv.org/abs/2509.24389) | [ğŸ¤— HuggingFace](https://huggingface.co/collections/inclusionAI/llada) |

</div>

---

## ğŸš€ Quick Start

```bash
# Start with the theory
cd "00_Diffusion_Theory"
cat README.md

# Then explore each paper in order
cd "../01_Simple_and_Effective_Masked_Diffusion_Language_Models"
cd "../02_Block_Diffusion_Interpolating_Between_AR_and_Diffusion"
cd "../03_Dream_7B_Diffusion_Large_Language_Models"
cd "../04_LLaDA_MoE_Sparse_MoE_Diffusion_Language_Model"

```

---

## ğŸ’¡ Key Insights

<div align="center">

```
.--------------------------------------------------------------------------.
|                                                                          |
|                     ğŸŒŸ THE BIG PICTURE ğŸŒŸ                                |
|                                                                          |
|  +----------------+  +----------------+  +----------------+             |
|  |                |  |                |  |                |             |
|  |   PARALLEL     |  |   REVISABLE    |  |   EFFICIENT    |             |
|  |   GENERATION   |  |   OUTPUTS      |  |   WITH MoE     |             |
|  |                |  |                |  |                |             |
|  |  All tokens    |  |  Fix mistakes  |  |  Sparse        |             |
|  |  at once       |  |  iteratively   |  |  computation   |             |
|  |                |  |                |  |                |             |
|  +-------+--------+  +-------+--------+  +-------+--------+             |
|          |                   |                   |                      |
|          +-------------------+-------------------+                      |
|                              |                                          |
|                              â–¼                                          |
|                    +-----------------+                                  |
|                    |                 |                                  |
|                    |  THE FUTURE OF  |                                  |
|                    |  LANGUAGE AI    |                                  |
|                    |                 |                                  |
|                    +-----------------+                                  |
|                                                                          |
'--------------------------------------------------------------------------'

```

</div>

---

<div align="center">

### ğŸ“œ License

*Educational purposes only. All papers belong to their respective authors.*

---

**Made with ğŸ’œ for the Machine Learning Community**

<br>

*"The best way to predict the future is to invent it."*

</div>

