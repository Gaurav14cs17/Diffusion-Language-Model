<div align="center">

# â¬…ï¸ Reverse Process

<img src="https://readme-typing-svg.demolab.com?font=Fira+Code&weight=500&size=24&duration=3000&pause=1000&color=818CF8&center=true&vCenter=true&width=500&lines=Learning+to+Denoise;From+Noise+to+Data" alt="Typing SVG" />

[â† Marginal Distributions](../06%20Marginal%20Distributions/) Â· **Page 7 of 10** Â· [Next: Training Objective â†’](../08%20Training%20Objective/)

<img src="https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png" alt="line" width="100%">

</div>

## ğŸ¯ Overview

The **reverse process** learns to undo the forward diffusion â€” gradually denoising pure noise back into data samples!

<p align="center">
  <img src="https://raw.githubusercontent.com/Gaurav14cs17/Diffusion-Language-Model/main/00_Diffusion_Theory/07%20Reverse%20Process/reverse_process.svg" alt="Reverse Process Diagram" width="100%">
</p>

---

## ğŸ“ Step 1: The Challenge

### What We Want

Given $x\_t$ (noisy), generate $x\_{t-1}$ (less noisy).

### The Problem

```math
p(x_{t-1} \mid x_t) = \frac{p(x_t \mid x_{t-1}) p(x_{t-1})}{p(x_t)}
```

We need $p(x\_{t-1})$ â€” the marginal distribution of data at step $t-1$!

This is **intractable** because:
1. $p(x\_0) = p\_{\text{data}}$ is unknown
2. $p(x\_t) = \int p(x\_t \mid x\_0) p(x\_0) dx\_0$ is intractable

### The Solution

**Learn** $p\_\theta(x\_{t-1} \mid x\_t)$ with a neural network!

```
+-------------------------------------------------------------+
|  Forward:  Known, fixed (Gaussian transitions)              |
|  Reverse:  Unknown, learned (neural network)                |
+-------------------------------------------------------------+
```

---

## ğŸ“ Step 2: Parameterizing the Reverse Process

### Key Insight

For small $\beta\_t$, the reverse process is **also Gaussian**! (See Feller, 1949)

### Parameterization

```math
p_\theta(x_{t-1} \mid x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
```

### Variance Choices

| Choice | Formula | Notes |
|--------|---------|-------|
| **Fixed (DDPM)** | $\sigma\_t^2 = \beta\_t$ | Simple, works well |
| **Fixed (optimal)** | $\sigma\_t^2 = \tilde{\beta}\_t$ | Lower bound |
| **Learned** | $\sigma\_t^2 = \exp(v \log\beta\_t + (1-v)\log\tilde{\beta}\_t)$ | Interpolate |

### Focus: Learning the Mean

The mean $\mu\_\theta(x\_t, t)$ is what the network must predict!

---

## ğŸ“ Step 3: What Should the Mean Be?

### Target: The True Posterior

If we knew $x\_0$, the optimal reverse step would be:

```math
q(x_{t-1} \mid x_t, x_0) = \mathcal{N}(\tilde{\mu}_t, \tilde{\beta}_t I)
```

where:

```math
\tilde{\mu}_t = \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} x_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t} x_t
```

### The Problem

We don't know $x\_0$ during generation!

### The Solution

**Predict** $x\_0$ from $x\_t$:

```math
\hat{x}_0 = f_\theta(x_t, t)
```

Then substitute into $\tilde{\mu}\_t$!

---

## ğŸ“ Step 4: Three Parameterizations

### Option A: Predict $x\_0$ Directly

```math
\mu_\theta = \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \hat{x}_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t} x_t
```

### Option B: Predict $\epsilon$ (Most Common!)

Since $x\_0 = \frac{x\_t - \sqrt{1-\bar{\alpha}\_t}\epsilon}{\sqrt{\bar{\alpha}\_t}}$:

```math
\boxed{\mu_\theta = \frac{1}{\sqrt{\alpha_t}}\left(x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(x_t, t)\right)}
```

### Proof of Equivalence

Start with $\mu\_\theta$ in terms of $x\_0$:

```math
\mu_\theta = \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} x_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t} x_t
```

Substitute $x\_0 = \frac{x\_t - \sqrt{1-\bar{\alpha}\_t}\epsilon}{\sqrt{\bar{\alpha}\_t}}$:

After algebraic simplification (tedious but straightforward):

```math
\mu_\theta = \frac{1}{\sqrt{\alpha_t}}\left(x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon\right) \quad \checkmark
```

### Option C: Predict $v$ (Velocity)

Define: $v\_t = \sqrt{\bar{\alpha}\_t}\epsilon - \sqrt{1-\bar{\alpha}\_t}x\_0$

Then: $\epsilon = \sqrt{\bar{\alpha}\_t}v\_t + \sqrt{1-\bar{\alpha}\_t}x\_t/\sqrt{\bar{\alpha}\_t}$

(See Parameterization chapter for details)

---

## ğŸ“ Step 5: The Sampling Algorithm

### DDPM Sampling (Algorithm)

```python
# Input: Trained noise predictor Îµ_Î¸
# Output: Generated sample x_0

x_T ~ N(0, I)  # Start from pure noise

for t in [T, T-1, ..., 1]:
    # Predict noise
    Îµ = Îµ_Î¸(x_t, t)
    
    # Compute mean
    Î¼ = (1/âˆšÎ±_t) * (x_t - Î²_t/âˆš(1-á¾±_t) * Îµ)
    
    # Sample (add noise except at t=1)
    if t > 1:
        z ~ N(0, I)
        x_{t-1} = Î¼ + Ïƒ_t * z
    else:
        x_0 = Î¼
        
return x_0
```

### Why Add Noise?

Even though we're denoising, we add small noise $\sigma\_t z$ because:
1. Matches the true reverse posterior
2. Helps exploration / diversity
3. Only skip at $t=1$ to get clean final sample

### Visual

```
t=T        t=T-1      t=T-2       ...      t=1       t=0
âšª ----â–¶ ğŸŒ«ï¸ ----â–¶ ğŸŒ«ï¸ ----â–¶ ... ----â–¶ ğŸ“‰ ----â–¶ ğŸ“Š
noise    denoise   denoise            denoise   clean!
```

---

## ğŸ“ Step 6: Derivation of Reverse Variance

### True Posterior Variance

```math
\tilde{\beta}_t = \frac{(1-\bar{\alpha}_{t-1})\beta_t}{1-\bar{\alpha}_t}
```

### Proof

From completing the square in the posterior derivation:

Precision (inverse variance):

```math
\tilde{\beta}_t^{-1} = \frac{\alpha_t}{\beta_t} + \frac{1}{1-\bar{\alpha}_{t-1}}
= \frac{\alpha_t(1-\bar{\alpha}_{t-1}) + \beta_t}{\beta_t(1-\bar{\alpha}_{t-1})}
```

Using $\alpha\_t + \beta\_t = 1$:

```math
= \frac{\alpha_t - \alpha_t\bar{\alpha}_{t-1} + 1 - \alpha_t}{\beta_t(1-\bar{\alpha}_{t-1})}
= \frac{1 - \bar{\alpha}_t}{\beta_t(1-\bar{\alpha}_{t-1})}
```

Therefore:

```math
\tilde{\beta}_t = \frac{\beta_t(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t} \quad \checkmark
```

### Two Common Choices

| $\sigma\_t^2$ | Formula | When to Use |
|:------------:|---------|-------------|
| $\beta\_t$ | Fixed, simple | Default (DDPM) |
| $\tilde{\beta}\_t$ | $\frac{(1-\bar{\alpha}\_{t-1})\beta\_t}{1-\bar{\alpha}\_t}$ | Theoretically optimal |

Both give similar results for well-trained models!

---

## ğŸ”‘ Summary

<div align="center">

| Component | Formula |
|-----------|---------|
| **Reverse transition** | $p\_\theta(x\_{t-1} \mid x\_t) = \mathcal{N}(\mu\_\theta, \sigma\_t^2 I)$ |
| **Mean (Îµ-pred)** | $\mu\_\theta = \frac{1}{\sqrt{\alpha\_t}}(x\_t - \frac{\beta\_t}{\sqrt{1-\bar{\alpha}\_t}}\epsilon\_\theta)$ |
| **Variance** | $\sigma\_t^2 = \beta\_t$ or $\tilde{\beta}\_t$ |

</div>

---

<div align="center">

<img src="https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png" alt="line" width="100%">

[â† Marginal Distributions](../06%20Marginal%20Distributions/) Â· **Page 7 of 10** Â· [Next: Training Objective â†’](../08%20Training%20Objective/)

</div>
