# ğŸŒŠ Diffusion Theory

<div align="center">

[![GitHub](https://img.shields.io/badge/GitHub-Gaurav14cs17-181717?style=for-the-badge&logo=github)](https://github.com/Gaurav14cs17)
[![Stars](https://img.shields.io/github/stars/Gaurav14cs17?style=for-the-badge)](https://github.com/Gaurav14cs17)

<img src="https://readme-typing-svg.demolab.com?font=Fira+Code&weight=600&size=28&duration=3000&pause=1000&color=6366F1&center=true&vCenter=true&multiline=true&repeat=true&width=600&height=100&lines=Diffusion+Theory;From+First+Principles+to+Implementation" alt="Typing SVG" />

*"Gradually destroy, then learn to create."*

<img src="https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png" alt="line" width="100%">

</div>

## ğŸ¯ What You'll Learn

<table>
<tr>
<td width="50%">

### ğŸ”¬ Mathematical Foundations
- Gaussian transitions & proofs
- Markov chain properties
- Variance preservation
- Score functions

</td>
<td width="50%">

### ğŸš€ Practical Implementation
- Training objectives (ELBO â†’ MSE)
- Sampling algorithms (DDPM, DDIM)
- Parameterization strategies
- Noise schedules

</td>
</tr>
</table>

---

## ğŸ“š The Big Picture

<div align="center">

```
+==============================================================================+

|                                                                              |
|   DATA                            FORWARD                            NOISE  |
|    xâ‚€  ============================================================â–¶  x_T   |
|   ğŸ“Š                        (Fixed, adds noise)                        ğŸŒ«ï¸   |
|                                                                              |
|    xâ‚€  â—€============================================================  x_T   |
|   ğŸ“Š                       (Learned, removes noise)                    ğŸŒ«ï¸   |
|                                REVERSE                                       |
|                                                                              |
+==============================================================================+

```

</div>

---

## ğŸ—‚ï¸ Course Contents

<div align="center">

| # | Topic | Description | Key Equation |
|:-:|-------|-------------|:------------:|
| ğŸ“‚ | [**01 Data Space**](./01%20Data%20Space/) | The manifold where data lives | $p\_{\text{data}}(x)$ |
| ğŸ“‚ | [**02 Probability Assumptions**](./02%20Probability%20Assumptions/) | Markov + Gaussian foundations | $q(x\_t \mid x\_{t-1})$ |
| ğŸ“‚ | [**03 Gaussian Transition**](./03%20Gaussian%20Transition%20Derivation/) | Step-by-step derivation | $x\_t = \sqrt{\alpha\_t} x\_{t-1} + \sqrt{\beta\_t} \epsilon$ |
| ğŸ“‚ | [**04 Forward Process**](./04%20Forward%20Process/) | How noise destroys data | $x\_t = \sqrt{\bar{\alpha}\_t} x\_0 + \sqrt{1-\bar{\alpha}\_t} \epsilon$ |
| ğŸ“‚ | [**05 Noise Schedule**](./05%20Noise%20Schedule/) | Linear, cosine, learned | $\beta\_t \in [\beta\_{\min}, \beta\_{\max}]$ |
| ğŸ“‚ | [**06 Marginal Distributions**](./06%20Marginal%20Distributions/) | Skip steps, train efficiently | $q(x\_t \mid x\_0) = \mathcal{N}(\sqrt{\bar{\alpha}\_t} x\_0, (1-\bar{\alpha}\_t) I)$ |
| ğŸ“‚ | [**07 Reverse Process**](./07%20Reverse%20Process/) | Learn to denoise | $p\_\theta(x\_{t-1} \mid x\_t)$ |
| ğŸ“‚ | [**08 Training Objective**](./08%20Training%20Objective/) | ELBO â†’ Simple MSE | $\mathcal{L} = \mathbb{E}[\|\epsilon - \epsilon\_\theta\|^2]$ |
| ğŸ“‚ | [**09 Parameterization**](./09%20Parameterization/) | Îµ, xâ‚€, or v prediction | All equivalent! |
| ğŸ“‚ | [**10 Sampling**](./10%20Sampling/) | DDPM, DDIM, DPM-Solver | Fast generation |

</div>

---

## ğŸ§® Core Equations

<div align="center">

### Forward Process (Noise Addition)

</div>

```math
\boxed{x_t = \sqrt{\bar{\alpha}_t} \cdot x_0 + \sqrt{1-\bar{\alpha}_t} \cdot \epsilon}

```

<div align="center">

### Reverse Process (Denoising)

</div>

```math
\boxed{\mu_\theta = \frac{1}{\sqrt{\alpha_t}}\left(x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(x_t, t)\right)}

```

<div align="center">

### Training Loss

</div>

```math
\boxed{\mathcal{L}_{\text{simple}} = \mathbb{E}_{t, x_0, \epsilon}\left[\|\epsilon - \epsilon_\theta(x_t, t)\|^2\right]}

```

---

## ğŸ¯ Learning Path

<div align="center">

```mermaid
graph LR
    A[ğŸ“Š Data Space] --> B[ğŸ“ Probability]
    B --> C[ğŸ”¢ Gaussian]
    C --> D[â¡ï¸ Forward]
    D --> E[ğŸ“ˆ Schedule]
    E --> F[ğŸ“‰ Marginal]
    F --> G[â¬…ï¸ Reverse]
    G --> H[ğŸ¯ Training]
    H --> I[âš™ï¸ Params]
    I --> J[ğŸ² Sampling]
    
    style A fill:#818cf8
    style B fill:#818cf8
    style C fill:#818cf8
    style D fill:#6366f1
    style E fill:#6366f1
    style F fill:#6366f1
    style G fill:#4f46e5
    style H fill:#4f46e5
    style I fill:#4f46e5
    style J fill:#4338ca

```

</div>

---

## ğŸ”‘ Key Insights

<table>
<tr>
<td width="50%">

### âœ… Why Diffusion Works

| Property | Benefit |
|----------|---------|
| ğŸ¯ Tractable | Closed-form marginals |
| ğŸ§  Flexible | Powerful neural networks |
| âš–ï¸ Stable | Small Gaussian steps |
| ğŸ¨ Expressive | Models complex distributions |

</td>
<td width="50%">

### âš–ï¸ The Trade-off

| More Steps | Fewer Steps |
|:---------:|:----------:|
| âœ… Easier learning | âŒ Harder learning |
| âœ… Better quality | âŒ Lower quality |
| âŒ Slow sampling | âœ… Fast sampling |

</td>
</tr>
</table>

---

## ğŸ“ Notation Reference

<div align="center">

| Symbol | Meaning | Symbol | Meaning |
|:------:|---------|:------:|---------|
| $x\_0$ | Clean data | $\epsilon$ | Standard Gaussian noise |
| $x\_t$ | Noisy data at step $t$ | $\epsilon\_\theta$ | Predicted noise |
| $x\_T$ | Pure noise | $q(\cdot)$ | Forward process |
| $\alpha\_t$ | Signal retention | $p\_\theta(\cdot)$ | Reverse process |
| $\beta\_t$ | Noise variance | $\bar{\alpha}\_t$ | Cumulative $\prod \alpha\_s$ |

</div>

---

## ğŸ“– References

<div align="center">

| Paper | Year | Contribution |
|-------|:----:|--------------|
| **DDPM** (Ho et al.) | 2020 | Denoising Diffusion Probabilistic Models |
| **Score SDE** (Song et al.) | 2021 | Score-Based Generative Modeling |
| **Improved DDPM** (Nichol & Dhariwal) | 2021 | Better architectures & schedules |
| **DDIM** (Song et al.) | 2020 | Deterministic & fast sampling |

</div>

---

## ğŸš€ Applications

<div align="center">

| Domain | Examples |
|:------:|----------|
| ğŸ–¼ï¸ **Images** | DALL-E, Stable Diffusion, Midjourney |
| ğŸµ **Audio** | AudioLDM, Riffusion, MusicGen |
| ğŸ¬ **Video** | Sora, Gen-2, Runway |
| ğŸ“ **Text** | Diffusion-LM, MDLM, SEDD |
| ğŸ§¬ **Science** | Drug discovery, protein design |
| ğŸ® **3D** | DreamFusion, Point-E, Magic3D |

</div>

---

## ğŸ‘¤ Author

<div align="center">

<img src="https://readme-typing-svg.demolab.com?font=Fira+Code&weight=500&size=22&duration=3000&pause=1000&color=6366F1&center=true&vCenter=true&width=435&lines=Gaurav+Goswami;ML+%26+AI+Researcher" alt="Author" />

[![GitHub](https://img.shields.io/badge/Follow-Gaurav14cs17-181717?style=for-the-badge&logo=github)](https://github.com/Gaurav14cs17)
[![LinkedIn](https://img.shields.io/badge/Connect-LinkedIn-0A66C2?style=for-the-badge&logo=linkedin)](https://linkedin.com)

</div>

---

<div align="center">

<img src="https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png" alt="line" width="100%">

### ğŸŒŸ Start Your Journey

**[ğŸ“‚ Begin with Data Space â†’](./01%20Data%20Space/)**

<br>

â­ **Star this repo if you find it helpful!** â­

<img src="https://readme-typing-svg.demolab.com?font=Fira+Code&size=14&duration=4000&pause=1000&color=6366F1&center=true&vCenter=true&width=435&lines=Happy+Learning!+%F0%9F%8E%89;Contributions+Welcome!+%F0%9F%A4%9D" alt="Footer" />

</div>
