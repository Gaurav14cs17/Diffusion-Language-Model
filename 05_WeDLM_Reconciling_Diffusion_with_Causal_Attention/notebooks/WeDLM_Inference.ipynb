{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ‚ö° WeDLM Inference\n",
        "\n",
        "**Key Formula - Adjusted Entropy:**\n",
        "$$\\tilde{H}_j = H(P_j) + \\lambda(j - j_{\\min}), \\quad \\text{Fill if } \\tilde{H}_j < \\tau$$\n",
        "\n",
        "## üìã Quick Start\n",
        "Run cells 1-3 for a quick demo of WeDLM parallel decoding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1Ô∏è‚É£ Install Dependencies & Clone Repo\n",
        "!pip install -q torch transformers accelerate\n",
        "\n",
        "# Clone the repository (for wedlm package)\n",
        "import os\n",
        "if not os.path.exists('05_WeDLM_Reconciling_Diffusion_with_Causal_Attention'):\n",
        "    !git clone https://github.com/Gaurav14cs17/05_WeDLM_Reconciling_Diffusion_with_Causal_Attention.git\n",
        "\n",
        "# Add to Python path\n",
        "import sys\n",
        "sys.path.insert(0, '05_WeDLM_Reconciling_Diffusion_with_Causal_Attention')\n",
        "\n",
        "# Try to import wedlm package\n",
        "try:\n",
        "    from wedlm import LLM, SamplingParams\n",
        "    WEDLM_AVAILABLE = True\n",
        "    print(\"‚úÖ wedlm package imported successfully!\")\n",
        "except ImportError as e:\n",
        "    WEDLM_AVAILABLE = False\n",
        "    print(f\"‚ö†Ô∏è wedlm import failed: {e}\")\n",
        "    print(\"Using standalone implementation...\")\n",
        "\n",
        "# 2Ô∏è‚É£ Load Model (standalone fallback)\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "MASK_TOKEN = \"<|mask|>\"\n",
        "W, TAU, LAMBDA = 16, 0.4, 0.02  # Window size, threshold, position penalty\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL, trust_remote_code=True,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32).to(device)\n",
        "if MASK_TOKEN not in tokenizer.get_vocab():\n",
        "    tokenizer.add_special_tokens({\"additional_special_tokens\": [MASK_TOKEN]})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "MASK_ID = tokenizer.convert_tokens_to_ids(MASK_TOKEN)\n",
        "print(f\"‚úÖ Model loaded on {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate(prompt, max_tokens=100):\n",
        "    model.eval()\n",
        "    gen = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)[0].tolist()\n",
        "    win, flags = [MASK_ID]*W, [True]*W\n",
        "    steps, toks = 0, 0\n",
        "    \n",
        "    while toks < max_tokens:\n",
        "        steps += 1\n",
        "        logits = model(torch.tensor([gen + win], device=device)).logits[0]\n",
        "        idx = [i for i,f in enumerate(flags) if f]\n",
        "        if not idx: break\n",
        "        \n",
        "        mlogits = torch.stack([logits[len(gen)+i-1] for i in idx])\n",
        "        probs = F.softmax(mlogits, dim=-1)\n",
        "        H = -(probs * torch.log(probs + 1e-10)).sum(-1)\n",
        "        pos = torch.tensor(idx, device=device, dtype=torch.float)\n",
        "        Hadj = H + LAMBDA * (pos - pos[0])\n",
        "        \n",
        "        fill = (Hadj < TAU).nonzero(as_tuple=True)[0]\n",
        "        if len(fill) == 0: fill = Hadj.argmin().unsqueeze(0)\n",
        "        \n",
        "        for k in fill.tolist():\n",
        "            win[idx[k]] = mlogits[k].argmax().item()\n",
        "            flags[idx[k]] = False\n",
        "        \n",
        "        commit = next((i for i,f in enumerate(flags) if f), len(win))\n",
        "        if commit == 0: commit = 1\n",
        "        gen.extend(win[:commit])\n",
        "        toks += commit\n",
        "        if tokenizer.eos_token_id in win[:commit]: break\n",
        "        win = win[commit:] + [MASK_ID]*commit\n",
        "        flags = flags[commit:] + [True]*commit\n",
        "    \n",
        "    return tokenizer.decode(gen, skip_special_tokens=True), {\"steps\": steps, \"tokens\": toks, \"tok/step\": toks/steps}\n",
        "\n",
        "print(\"‚úÖ Generator ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo\n",
        "prompts = [\"Solve: 15 √ó 7 + 23 = \", \"The capital of France is\", \"def fibonacci(n):\"]\n",
        "for p in prompts:\n",
        "    t0 = time.time()\n",
        "    out, stats = generate(p, 50)\n",
        "    print(f\"\\n{'='*50}\\nPrompt: {p}\\nOutput: {out[len(p):]}\\nStats: {stats}, Time: {time.time()-t0:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìö Detailed Version (Alternative Implementation)\n",
        "\n",
        "The cells above provide a compact demo. Below is a more detailed version with extensive comments.\n",
        "\n",
        "## üìê The Algorithm\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                    WeDLM Inference Flow                         ‚îÇ\n",
        "‚îÇ                                                                 ‚îÇ\n",
        "‚îÇ  Prefix (KV Cache)          Window (Processing)                ‚îÇ\n",
        "‚îÇ  [The][quick][brown]        [M][M][M][M][M][M][M][M]           ‚îÇ\n",
        "‚îÇ                              ‚îÇ                                  ‚îÇ\n",
        "‚îÇ                              ‚ñº                                  ‚îÇ\n",
        "‚îÇ                    1. Forward Pass (causal attention)          ‚îÇ\n",
        "‚îÇ                              ‚îÇ                                  ‚îÇ\n",
        "‚îÇ                              ‚ñº                                  ‚îÇ\n",
        "‚îÇ                    2. Compute Entropy H(P) for each mask       ‚îÇ\n",
        "‚îÇ                              ‚îÇ                                  ‚îÇ\n",
        "‚îÇ                              ‚ñº                                  ‚îÇ\n",
        "‚îÇ                    3. Fill positions where HÃÉ < threshold       ‚îÇ\n",
        "‚îÇ                              ‚îÇ                                  ‚îÇ\n",
        "‚îÇ                              ‚ñº                                  ‚îÇ\n",
        "‚îÇ                    4. Commit prefix, slide window              ‚îÇ\n",
        "‚îÇ                                                                 ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1Ô∏è‚É£ Install Dependencies & Setup\n",
        "!pip install -q torch transformers accelerate\n",
        "!pip install -q flash-attn --no-build-isolation 2>/dev/null || echo \"FlashAttn not available (optional)\"\n",
        "\n",
        "# Clone repo and setup wedlm import\n",
        "import os, sys\n",
        "if not os.path.exists('05_WeDLM_Reconciling_Diffusion_with_Causal_Attention'):\n",
        "    !git clone https://github.com/Gaurav14cs17/05_WeDLM_Reconciling_Diffusion_with_Causal_Attention.git\n",
        "sys.path.insert(0, '05_WeDLM_Reconciling_Diffusion_with_Causal_Attention')\n",
        "\n",
        "# Import wedlm\n",
        "try:\n",
        "    from wedlm import LLM, SamplingParams\n",
        "    print(\"‚úÖ wedlm package imported!\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è Using standalone implementation\")\n",
        "\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 2Ô∏è‚É£ Load Model (Using Qwen as base for demo, replace with WeDLM when available)\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"  # Replace with \"tencent/WeDLM-8B-Instruct\" for full model\n",
        "\n",
        "print(f\"Loading {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, trust_remote_code=True,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ").to(device)\n",
        "\n",
        "# Add mask token\n",
        "MASK_TOKEN = \"<|mask|>\"\n",
        "if MASK_TOKEN not in tokenizer.get_vocab():\n",
        "    tokenizer.add_special_tokens({\"additional_special_tokens\": [MASK_TOKEN]})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "MASK_ID = tokenizer.convert_tokens_to_ids(MASK_TOKEN)\n",
        "\n",
        "print(f\"‚úÖ Model loaded! Mask token ID: {MASK_ID}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3Ô∏è‚É£ WeDLM Core Functions\n",
        "\n",
        "def compute_entropy(logits):\n",
        "    \"\"\"\n",
        "    Compute entropy: H(P) = -Œ£ p_i log(p_i)\n",
        "    \n",
        "    Low entropy = high confidence = safe to commit\n",
        "    \"\"\"\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    return -(probs * torch.log(probs + 1e-10)).sum(dim=-1)\n",
        "\n",
        "def select_positions_to_fill(entropy, remaining_indices, threshold=0.4, pos_penalty=0.02):\n",
        "    \"\"\"\n",
        "    Select which mask positions to fill based on adjusted entropy.\n",
        "    \n",
        "    Adjusted entropy: HÃÉ_j = H(P_j) + Œª(j - j_min)\n",
        "    \n",
        "    Args:\n",
        "        entropy: Raw entropy values\n",
        "        remaining_indices: Position indices in window\n",
        "        threshold: Fill if HÃÉ < threshold\n",
        "        pos_penalty: Œª - penalty for later positions\n",
        "    \"\"\"\n",
        "    positions = torch.tensor(remaining_indices, device=entropy.device, dtype=torch.float)\n",
        "    base_pos = positions[0]\n",
        "    \n",
        "    # Position penalty encourages left-to-right decoding\n",
        "    adjusted = entropy + pos_penalty * (positions - base_pos)\n",
        "    \n",
        "    # Select low-entropy positions\n",
        "    selected = (adjusted < threshold).nonzero(as_tuple=True)[0]\n",
        "    \n",
        "    if len(selected) == 0:\n",
        "        # Fallback: select minimum entropy position\n",
        "        selected = adjusted.argmin().unsqueeze(0)\n",
        "    \n",
        "    return selected.tolist()\n",
        "\n",
        "print(\"‚úÖ Core functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4Ô∏è‚É£ WeDLM Generation Function\n",
        "\n",
        "@torch.no_grad()\n",
        "def wedlm_generate(\n",
        "    model, tokenizer, prompt,\n",
        "    max_new_tokens=100,\n",
        "    window_size=16,\n",
        "    entropy_threshold=0.4,\n",
        "    pos_penalty=0.02,\n",
        "    temperature=0.0,\n",
        "    verbose=False\n",
        "):\n",
        "    \"\"\"\n",
        "    WeDLM Streaming Parallel Decoding\n",
        "    \n",
        "    Args:\n",
        "        model: Language model\n",
        "        tokenizer: Tokenizer\n",
        "        prompt: Input text\n",
        "        max_new_tokens: Max tokens to generate\n",
        "        window_size: Sliding window size (W)\n",
        "        entropy_threshold: œÑ - threshold for parallel filling\n",
        "        pos_penalty: Œª - position penalty factor\n",
        "        temperature: Sampling temperature (0 = greedy)\n",
        "        verbose: Print step-by-step progress\n",
        "    \n",
        "    Returns:\n",
        "        Generated text, stats dictionary\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    mask_id = tokenizer.convert_tokens_to_ids(MASK_TOKEN)\n",
        "    eos_id = tokenizer.eos_token_id\n",
        "    \n",
        "    # Encode prompt\n",
        "    prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)[0].tolist()\n",
        "    generated = prompt_ids.copy()\n",
        "    \n",
        "    # Initialize window with masks\n",
        "    window = [mask_id] * window_size\n",
        "    window_mask_flags = [True] * window_size  # True = mask\n",
        "    \n",
        "    tokens_generated = 0\n",
        "    steps = 0\n",
        "    total_filled = 0\n",
        "    \n",
        "    while tokens_generated < max_new_tokens:\n",
        "        steps += 1\n",
        "        \n",
        "        # Build input: prefix + window\n",
        "        input_ids = torch.tensor([generated + window], device=device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(input_ids)\n",
        "        logits = outputs.logits[0]  # [seq_len, vocab]\n",
        "        \n",
        "        # Get logits for mask positions in window\n",
        "        prefix_len = len(generated)\n",
        "        \n",
        "        # Find remaining mask positions\n",
        "        mask_indices = [i for i, is_mask in enumerate(window_mask_flags) if is_mask]\n",
        "        \n",
        "        if not mask_indices:\n",
        "            break\n",
        "        \n",
        "        # Get logits for mask positions (offset by -1 for next-token prediction)\n",
        "        mask_logits = torch.stack([logits[prefix_len + i - 1] for i in mask_indices])\n",
        "        \n",
        "        # Compute entropy\n",
        "        entropy = compute_entropy(mask_logits)\n",
        "        \n",
        "        # Select positions to fill\n",
        "        fill_indices = select_positions_to_fill(\n",
        "            entropy, mask_indices, entropy_threshold, pos_penalty\n",
        "        )\n",
        "        \n",
        "        # Sample tokens\n",
        "        for idx in fill_indices:\n",
        "            pos = mask_indices[idx]\n",
        "            pos_logits = mask_logits[idx]\n",
        "            \n",
        "            if temperature > 0:\n",
        "                probs = F.softmax(pos_logits / temperature, dim=-1)\n",
        "                token = torch.multinomial(probs, 1).item()\n",
        "            else:\n",
        "                token = pos_logits.argmax().item()\n",
        "            \n",
        "            window[pos] = token\n",
        "            window_mask_flags[pos] = False\n",
        "            total_filled += 1\n",
        "        \n",
        "        # Find committed prefix (consecutive non-masks from start)\n",
        "        commit_count = 0\n",
        "        for i in range(len(window)):\n",
        "            if not window_mask_flags[i]:\n",
        "                commit_count += 1\n",
        "            else:\n",
        "                break\n",
        "        \n",
        "        if commit_count == 0:\n",
        "            commit_count = 1  # Force progress\n",
        "        \n",
        "        # Commit to output\n",
        "        committed = window[:commit_count]\n",
        "        generated.extend(committed)\n",
        "        tokens_generated += commit_count\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"Step {steps}: Filled {len(fill_indices)}, Committed {commit_count}\")\n",
        "        \n",
        "        # Check for EOS\n",
        "        if eos_id in committed:\n",
        "            break\n",
        "        \n",
        "        # Slide window\n",
        "        window = window[commit_count:] + [mask_id] * commit_count\n",
        "        window_mask_flags = window_mask_flags[commit_count:] + [True] * commit_count\n",
        "    \n",
        "    # Decode\n",
        "    output_text = tokenizer.decode(generated, skip_special_tokens=True)\n",
        "    \n",
        "    stats = {\n",
        "        \"steps\": steps,\n",
        "        \"tokens_generated\": tokens_generated,\n",
        "        \"avg_tokens_per_step\": tokens_generated / steps if steps > 0 else 0,\n",
        "        \"total_filled\": total_filled\n",
        "    }\n",
        "    \n",
        "    return output_text, stats\n",
        "\n",
        "print(\"‚úÖ Generation function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5Ô∏è‚É£ Demo: Math Problem\n",
        "print(\"=\" * 70)\n",
        "print(\"üìù DEMO: Math Problem Solving\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "prompt = \"Solve step by step: What is 15 √ó 7 + 23?\"\n",
        "\n",
        "start = time.time()\n",
        "output, stats = wedlm_generate(\n",
        "    model, tokenizer, prompt,\n",
        "    max_new_tokens=100,\n",
        "    entropy_threshold=0.5,\n",
        "    verbose=True\n",
        ")\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Output: {output[len(prompt):]}\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"üìä Statistics:\")\n",
        "print(f\"   ‚Ä¢ Tokens generated: {stats['tokens_generated']}\")\n",
        "print(f\"   ‚Ä¢ Forward passes: {stats['steps']}\")\n",
        "print(f\"   ‚Ä¢ Avg tokens/step: {stats['avg_tokens_per_step']:.2f}\")\n",
        "print(f\"   ‚Ä¢ Time: {elapsed:.2f}s\")\n",
        "print(f\"   ‚Ä¢ Speed: {stats['tokens_generated']/elapsed:.1f} tok/s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6Ô∏è‚É£ Compare: Different Entropy Thresholds\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üî¨ Experiment: Effect of Entropy Threshold\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "test_prompt = \"The capital of France is\"\n",
        "\n",
        "for threshold in [0.2, 0.4, 0.6, 0.8]:\n",
        "    output, stats = wedlm_generate(\n",
        "        model, tokenizer, test_prompt,\n",
        "        max_new_tokens=30,\n",
        "        entropy_threshold=threshold\n",
        "    )\n",
        "    print(f\"\\nœÑ = {threshold}:\")\n",
        "    print(f\"   Steps: {stats['steps']}, Avg tok/step: {stats['avg_tokens_per_step']:.2f}\")\n",
        "    print(f\"   Output: {output[len(test_prompt):50]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìö Summary\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "| Parameter | Default | Description |\n",
        "|-----------|---------|-------------|\n",
        "| `window_size` | 16 | Number of mask tokens to process in parallel |\n",
        "| `entropy_threshold` (œÑ) | 0.4 | Fill positions with HÃÉ < œÑ |\n",
        "| `pos_penalty` (Œª) | 0.02 | Penalty for later positions |\n",
        "\n",
        "### Speed-Quality Tradeoff\n",
        "\n",
        "- **Lower œÑ** ‚Üí More conservative, higher quality, fewer tokens/step\n",
        "- **Higher œÑ** ‚Üí More aggressive, faster, potential quality drop\n",
        "- **Higher Œª** ‚Üí Stronger left-to-right bias\n",
        "\n",
        "### Recommended Settings\n",
        "\n",
        "| Use Case | œÑ | Œª | Expected Speedup |\n",
        "|----------|---|---|------------------|\n",
        "| Math/Code | 0.4-0.6 | 0.02 | 3-6√ó |\n",
        "| General QA | 0.3-0.4 | 0.02 | 1.5-2√ó |\n",
        "| Creative | 0.2-0.3 | 0.01 | 1-1.5√ó |\n",
        "\n",
        "---\n",
        "\n",
        "## üîó Resources\n",
        "\n",
        "- [Paper](https://arxiv.org/abs/2512.22737)\n",
        "- [Official Code](https://github.com/tencent/WeDLM)\n",
        "- [HuggingFace Models](https://huggingface.co/collections/tencent/wedlm)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
