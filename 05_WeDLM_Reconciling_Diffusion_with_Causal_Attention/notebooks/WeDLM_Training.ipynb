{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéì WeDLM Training Tutorial\n",
        "\n",
        "Fine-tune a pretrained AR model into WeDLM using **Causal Masked Language Modeling (CMLM)**:\n",
        "\n",
        "$$\\mathcal{L}_{\\text{CMLM}} = -\\mathbb{E}_{x} \\left[\\sum_{j \\in \\mathcal{M}} \\log P_\\theta(x_j \\mid x_{<j} \\cap \\mathcal{M}^c)\\right]$$\n",
        "\n",
        "## üìã Contents\n",
        "1. Environment Setup\n",
        "2. Configuration  \n",
        "3. Data Preparation with Span Masking\n",
        "4. Model Loading\n",
        "5. Training Loop\n",
        "6. Test Generation\n",
        "7. Save Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1Ô∏è‚É£ Install Dependencies & Clone Repo\n",
        "!pip install -q torch transformers datasets tqdm accelerate\n",
        "\n",
        "# Clone the repository (for wedlm package)\n",
        "import os\n",
        "if not os.path.exists('05_WeDLM_Reconciling_Diffusion_with_Causal_Attention'):\n",
        "    !git clone https://github.com/Gaurav14cs17/05_WeDLM_Reconciling_Diffusion_with_Causal_Attention.git\n",
        "    \n",
        "# Add to Python path\n",
        "import sys\n",
        "sys.path.insert(0, '05_WeDLM_Reconciling_Diffusion_with_Causal_Attention')\n",
        "\n",
        "# Verify wedlm import\n",
        "try:\n",
        "    from wedlm import LLM, SamplingParams\n",
        "    print(\"‚úÖ wedlm package imported successfully!\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö†Ô∏è wedlm import failed: {e}\")\n",
        "    print(\"Continuing with standalone implementation...\")\n",
        "\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Configuration\n",
        "MODEL = \"Qwen/Qwen2.5-0.5B\"\n",
        "MAX_LEN = 256\n",
        "BATCH = 4\n",
        "EPOCHS = 2\n",
        "LR = 2e-5\n",
        "MASK_TOKEN = \"<|mask|>\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Span Masking Function\n",
        "def span_mask(ids, mask_id, ratio=0.3):\n",
        "    n = len(ids)\n",
        "    masked = ids.clone()\n",
        "    flags = torch.zeros(n, dtype=torch.bool)\n",
        "    target = int(n * ratio)\n",
        "    count = 0\n",
        "    while count < target:\n",
        "        span = min(np.random.geometric(0.3), n-1)\n",
        "        start = np.random.randint(0, n - span + 1)\n",
        "        for i in range(start, start + span):\n",
        "            if not flags[i]:\n",
        "                masked[i] = mask_id\n",
        "                flags[i] = True\n",
        "                count += 1\n",
        "                if count >= target: break\n",
        "    return masked, ids, flags\n",
        "\n",
        "# Dataset\n",
        "class MLMDataset(Dataset):\n",
        "    def __init__(self, tok, texts, max_len, mask_id):\n",
        "        self.tok, self.texts, self.max_len, self.mask_id = tok, texts, max_len, mask_id\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, i):\n",
        "        enc = self.tok(self.texts[i], truncation=True, max_length=self.max_len, \n",
        "                       padding=\"max_length\", return_tensors=\"pt\")\n",
        "        ids = enc[\"input_ids\"].squeeze(0)\n",
        "        masked, labels, flags = span_mask(ids, self.mask_id, np.random.uniform(0.1, 0.5))\n",
        "        return {\"input_ids\": masked, \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "                \"labels\": labels, \"mask_flags\": flags}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Model & Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
        "if MASK_TOKEN not in tokenizer.get_vocab():\n",
        "    tokenizer.add_special_tokens({\"additional_special_tokens\": [MASK_TOKEN]})\n",
        "mask_id = tokenizer.convert_tokens_to_ids(MASK_TOKEN)\n",
        "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL, trust_remote_code=True,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = model.to(device)\n",
        "print(\"‚úÖ Model loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Data\n",
        "data = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "texts = [t for t in data[\"text\"] if len(t.strip()) > 50][:500]\n",
        "dataset = MLMDataset(tokenizer, texts, MAX_LEN, mask_id)\n",
        "loader = DataLoader(dataset, batch_size=BATCH, shuffle=True)\n",
        "print(f\"‚úÖ {len(texts)} training samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "opt = AdamW(model.parameters(), lr=LR)\n",
        "use_amp = torch.cuda.is_available()\n",
        "scaler = torch.amp.GradScaler('cuda') if use_amp else None\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total = 0\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\")\n",
        "    for batch in pbar:\n",
        "        ids = batch[\"input_ids\"].to(device)\n",
        "        mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        flags = batch[\"mask_flags\"].to(device)\n",
        "        \n",
        "        if use_amp:\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                logits = model(ids, attention_mask=mask).logits\n",
        "                shift_logits = logits[:, :-1, :].reshape(-1, logits.size(-1))\n",
        "                shift_labels = labels[:, 1:].reshape(-1)\n",
        "                shift_flags = flags[:, 1:].reshape(-1)\n",
        "                if shift_flags.sum() > 0:\n",
        "                    loss = F.cross_entropy(shift_logits[shift_flags], shift_labels[shift_flags])\n",
        "                else:\n",
        "                    loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
        "        else:\n",
        "            logits = model(ids, attention_mask=mask).logits\n",
        "            shift_logits = logits[:, :-1, :].reshape(-1, logits.size(-1))\n",
        "            shift_labels = labels[:, 1:].reshape(-1)\n",
        "            shift_flags = flags[:, 1:].reshape(-1)\n",
        "            if shift_flags.sum() > 0:\n",
        "                loss = F.cross_entropy(shift_logits[shift_flags], shift_labels[shift_flags])\n",
        "            else:\n",
        "                loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
        "        \n",
        "        opt.zero_grad()\n",
        "        if scaler:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        \n",
        "        total += loss.item()\n",
        "        pbar.set_postfix({\"loss\": f\"{total/(pbar.n+1):.4f}\"})\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}: Loss = {total/len(loader):.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Training Complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Model\n",
        "model.save_pretrained(\"./wedlm_model\")\n",
        "tokenizer.save_pretrained(\"./wedlm_model\")\n",
        "print(\"‚úÖ Model saved to ./wedlm_model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìö Detailed Version (Alternative Implementation)\n",
        "\n",
        "The cells above provide a compact implementation. Below is a more detailed version with extensive comments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1Ô∏è‚É£ Environment Setup (Detailed Version)\n",
        "!pip install -q torch transformers datasets accelerate tqdm\n",
        "\n",
        "# Clone repo and setup wedlm import (if not already done)\n",
        "import os, sys\n",
        "if not os.path.exists('05_WeDLM_Reconciling_Diffusion_with_Causal_Attention'):\n",
        "    !git clone https://github.com/Gaurav14cs17/05_WeDLM_Reconciling_Diffusion_with_Causal_Attention.git\n",
        "sys.path.insert(0, '05_WeDLM_Reconciling_Diffusion_with_Causal_Attention')\n",
        "\n",
        "# Import wedlm\n",
        "try:\n",
        "    from wedlm import LLM, SamplingParams\n",
        "    print(\"‚úÖ wedlm package imported!\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è Using standalone implementation\")\n",
        "\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 2Ô∏è‚É£ Configuration\n",
        "class Config:\n",
        "    model_name = \"Qwen/Qwen2.5-0.5B\"  # Small model for demo (use larger for production)\n",
        "    max_seq_len = 256\n",
        "    batch_size = 4\n",
        "    gradient_accumulation_steps = 4\n",
        "    learning_rate = 2e-5\n",
        "    num_epochs = 2\n",
        "    mask_ratio_min = 0.1\n",
        "    mask_ratio_max = 0.5\n",
        "    mask_token = \"<|mask|>\"\n",
        "\n",
        "config = Config()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3Ô∏è‚É£ Random Span Masking Function\n",
        "def random_span_masking(input_ids, mask_token_id, mask_ratio=0.3, span_mean=3):\n",
        "    \"\"\"\n",
        "    Apply random span masking to input sequence.\n",
        "    \n",
        "    Original:  [The] [quick] [brown] [fox] [jumps]\n",
        "    Masked:    [The] [MASK] [MASK] [fox] [jumps]\n",
        "    \"\"\"\n",
        "    seq_len = len(input_ids)\n",
        "    num_to_mask = int(seq_len * mask_ratio)\n",
        "    \n",
        "    masked_ids = input_ids.clone()\n",
        "    mask_flags = torch.zeros(seq_len, dtype=torch.bool)\n",
        "    \n",
        "    positions_masked = 0\n",
        "    attempts = 0\n",
        "    \n",
        "    while positions_masked < num_to_mask and attempts < seq_len * 10:\n",
        "        attempts += 1\n",
        "        span_len = min(np.random.geometric(p=1/span_mean), seq_len - 1)\n",
        "        start = np.random.randint(0, seq_len - span_len + 1)\n",
        "        \n",
        "        for i in range(start, start + span_len):\n",
        "            if not mask_flags[i]:\n",
        "                masked_ids[i] = mask_token_id\n",
        "                mask_flags[i] = True\n",
        "                positions_masked += 1\n",
        "                if positions_masked >= num_to_mask:\n",
        "                    break\n",
        "    \n",
        "    return masked_ids, input_ids, mask_flags\n",
        "\n",
        "# 4Ô∏è‚É£ Dataset Class\n",
        "class CausalMLMDataset(Dataset):\n",
        "    def __init__(self, tokenizer, texts, max_length, mask_token_id):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts = texts\n",
        "        self.max_length = max_length\n",
        "        self.mask_token_id = mask_token_id\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx], truncation=True, max_length=self.max_length,\n",
        "            padding=\"max_length\", return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
        "        \n",
        "        mask_ratio = np.random.uniform(config.mask_ratio_min, config.mask_ratio_max)\n",
        "        masked_ids, labels, mask_flags = random_span_masking(input_ids, self.mask_token_id, mask_ratio)\n",
        "        \n",
        "        return {\"input_ids\": masked_ids, \"attention_mask\": attention_mask, \n",
        "                \"labels\": labels, \"mask_flags\": mask_flags}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5Ô∏è‚É£ Load Tokenizer and Data\n",
        "print(f\"Loading tokenizer: {config.model_name}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True)\n",
        "\n",
        "# Add mask token\n",
        "if config.mask_token not in tokenizer.get_vocab():\n",
        "    tokenizer.add_special_tokens({\"additional_special_tokens\": [config.mask_token]})\n",
        "mask_token_id = tokenizer.convert_tokens_to_ids(config.mask_token)\n",
        "print(f\"Mask token ID: {mask_token_id}\")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load dataset\n",
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "texts = [t for t in dataset[\"text\"] if len(t.strip()) > 50][:500]  # Subset for demo\n",
        "print(f\"Using {len(texts)} training examples\")\n",
        "\n",
        "train_dataset = CausalMLMDataset(tokenizer, texts, config.max_seq_len, mask_token_id)\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6Ô∏è‚É£ Load Model\n",
        "print(f\"Loading model: {config.model_name}\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.model_name, trust_remote_code=True,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = model.to(device)\n",
        "print(f\"Model loaded on {device}\")\n",
        "\n",
        "# 7Ô∏è‚É£ Loss Function\n",
        "def compute_cmlm_loss(model, batch, mask_token_id):\n",
        "    \"\"\"Compute loss only on masked positions with causal attention.\"\"\"\n",
        "    input_ids = batch[\"input_ids\"].to(device)\n",
        "    attention_mask = batch[\"attention_mask\"].to(device)\n",
        "    labels = batch[\"labels\"].to(device)\n",
        "    mask_flags = batch[\"mask_flags\"].to(device)\n",
        "    \n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n",
        "    \n",
        "    # Shift for next-token prediction\n",
        "    shift_logits = logits[:, :-1, :].reshape(-1, logits.size(-1))\n",
        "    shift_labels = labels[:, 1:].reshape(-1)\n",
        "    shift_mask = mask_flags[:, 1:].reshape(-1)\n",
        "    \n",
        "    # Loss only on masked positions\n",
        "    if shift_mask.sum() == 0:\n",
        "        return torch.tensor(0.0, device=device)\n",
        "    \n",
        "    return F.cross_entropy(shift_logits[shift_mask], shift_labels[shift_mask])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8Ô∏è‚É£ Training Loop\n",
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=config.learning_rate, weight_decay=0.01)\n",
        "use_amp = torch.cuda.is_available()\n",
        "scaler = torch.amp.GradScaler('cuda') if use_amp else None\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for epoch in range(config.num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    for step, batch in enumerate(progress_bar):\n",
        "        if use_amp:\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                loss = compute_cmlm_loss(model, batch, mask_token_id)\n",
        "                loss = loss / config.gradient_accumulation_steps\n",
        "        else:\n",
        "            loss = compute_cmlm_loss(model, batch, mask_token_id)\n",
        "            loss = loss / config.gradient_accumulation_steps\n",
        "        \n",
        "        if scaler:\n",
        "            scaler.scale(loss).backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "        \n",
        "        if (step + 1) % config.gradient_accumulation_steps == 0:\n",
        "            if scaler:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        total_loss += loss.item() * config.gradient_accumulation_steps\n",
        "        progress_bar.set_postfix({\"loss\": f\"{total_loss/(step+1):.4f}\"})\n",
        "    \n",
        "    print(f\"Epoch {epoch+1} - Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ TRAINING COMPLETE!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9Ô∏è‚É£ Test Generation (Simplified WeDLM Decoding)\n",
        "@torch.no_grad()\n",
        "def wedlm_generate(model, tokenizer, prompt, max_tokens=30, window_size=8, entropy_threshold=0.5):\n",
        "    \"\"\"Simplified WeDLM generation demo.\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    generated = prompt_ids[0].tolist()\n",
        "    window = [mask_token_id] * window_size\n",
        "    \n",
        "    for _ in range(max_tokens // window_size + 1):\n",
        "        input_ids = torch.tensor([generated + window], device=device)\n",
        "        logits = model(input_ids).logits[0]\n",
        "        \n",
        "        window_logits = logits[len(generated)-1:len(generated)+window_size-1]\n",
        "        probs = F.softmax(window_logits, dim=-1)\n",
        "        entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=-1)\n",
        "        \n",
        "        # Fill low-entropy positions\n",
        "        predicted = probs.argmax(dim=-1)\n",
        "        fill_mask = entropy < entropy_threshold\n",
        "        if not fill_mask.any():\n",
        "            fill_mask[0] = True\n",
        "        \n",
        "        for i in range(window_size):\n",
        "            if fill_mask[i]:\n",
        "                window[i] = predicted[i].item()\n",
        "        \n",
        "        # Commit prefix\n",
        "        commit = 0\n",
        "        for i, tok in enumerate(window):\n",
        "            if tok != mask_token_id:\n",
        "                commit += 1\n",
        "            else:\n",
        "                break\n",
        "        if commit == 0:\n",
        "            commit = 1\n",
        "        \n",
        "        generated.extend(window[:commit])\n",
        "        if tokenizer.eos_token_id in window[:commit]:\n",
        "            break\n",
        "        window = window[commit:] + [mask_token_id] * commit\n",
        "    \n",
        "    return tokenizer.decode(generated, skip_special_tokens=True)\n",
        "\n",
        "# Test\n",
        "print(\"=\" * 60)\n",
        "print(\"TESTING GENERATION\")\n",
        "print(\"=\" * 60)\n",
        "for prompt in [\"The quick brown\", \"Machine learning is\"]:\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    print(f\"Output: {wedlm_generate(model, tokenizer, prompt)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîü Save Model\n",
        "output_dir = \"./wedlm_finetuned\"\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"\\n‚úÖ Model saved to {output_dir}\")\n",
        "print(\"\\nüìö Training complete! Next: Use the Inference notebook for optimized generation.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
